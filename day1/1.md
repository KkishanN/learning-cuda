# Day 1: CUDA Learning




- **GPU Architecture:** GPUs consist of many independent workers (cores) designed to execute simple tasks in parallel. The focus is on maximizing throughput rather than minimizing the execution time of individual tasks.
- **CUDA System Model:** CUDA divides the computational system into two parts:
    1. **Host:** The CPU and its memory.
    2. **Device:** The GPU and its dedicated VRAM.
- **Physical Separation:** The host and device are connected via the PCI Express (PCIe) bus.
- **Bandwidth Considerations:** Internal GPU memory bandwidth is around 1 TB/s, while PCIe bandwidth is much lower (32–64 GB/s). Minimizing data transfers between the host and device is crucial for performance.
- **Programming Mindset Shift:** 
    - On CPUs, time-based loops (sequential execution) are commonly used.
    - On GPUs, it is more effective to think in terms of space—using grids of threads to process data in parallel.