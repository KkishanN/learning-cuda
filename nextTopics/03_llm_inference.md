# 03. LLM Inference Optimization

## ğŸ¯ Overview
Large Language Model (LLM) inference is the **hottest skill in GPU programming** right now. Master these techniques to land top-paying AI infrastructure roles.

---

## ğŸ§  LLM Inference Basics

### The Problem
- GPT-4: ~1.8 trillion parameters
- 70B model = 140GB in FP16
- Generation is **autoregressive** (sequential)

```
Input: "The quick brown"
Step 1: "The quick brown" â†’ "fox"
Step 2: "The quick brown fox" â†’ "jumps"
Step 3: "The quick brown fox jumps" â†’ "over"
...
Each step depends on previous!
```

---

## ğŸ”‘ Key Optimization Techniques

### 1. KV Cache
Store computed key-value pairs instead of recomputing:

```
Without KV Cache:
Step N: Compute attention for ALL N tokens

With KV Cache:
Step N: Compute attention for NEW token only
        Reuse K,V from steps 1 to N-1
        
Speedup: O(NÂ²) â†’ O(N)
```

Memory cost: ~2 Ã— layers Ã— hidden_size Ã— sequence_length Ã— batch_size

### 2. PagedAttention (vLLM)
Manage KV cache like virtual memory:

```
Traditional: Contiguous allocation per request
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request 1 KV Cache (reserved max)   â”‚ â† Wasted space!
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Request 2 KV Cache (reserved max)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PagedAttention: Block-based allocation
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ R1.1 â”‚ R1.2 â”‚ R2.1 â”‚ R1.3 â”‚ R2.2 â”‚ Free â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
         â†‘ Blocks allocated on-demand
```

Benefits:
- Near-zero waste
- 24Ã— more concurrent requests
- Dynamic sequence lengths

### 3. Continuous Batching
Don't wait for slowest request:

```
Traditional Batching:
Request 1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] (1000 tokens)
Request 2: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ........] (500 tokens, waiting)
Request 3: [â–ˆâ–ˆâ–ˆâ–ˆ............] (200 tokens, waiting)
           â†‘ All wait for Request 1

Continuous Batching:
Request 3 finishes â†’ immediately insert Request 4
No waiting, maximum GPU utilization!
```

### 4. Speculative Decoding
Use small model to draft, large model to verify:

```
Draft (7B model): "The quick brown fox" â†’ "jumps over the lazy"
Verify (70B model): Accept 4/5 tokens in one forward pass
                    
Result: 2-3Ã— speedup with no quality loss!
```

---

## ğŸ› ï¸ Key Tools

### vLLM
```python
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-2-7b-hf")
params = SamplingParams(temperature=0.7, max_tokens=100)

outputs = llm.generate(["Hello, how are"], params)
print(outputs[0].outputs[0].text)
```

Features:
- PagedAttention
- Continuous batching
- Tensor parallelism
- OpenAI-compatible API

### TensorRT-LLM
NVIDIA's optimized LLM inference:
- Custom attention kernels
- INT4/INT8 quantization
- Multi-GPU support
- Flash Attention integration

```bash
# Convert and build
python convert_checkpoint.py --model_dir llama-7b
trtllm-build --checkpoint_dir ./checkpoint --output_dir ./engine
```

### Text Generation Inference (TGI)
HuggingFace's production server:
- Flash Attention
- Quantization (bitsandbytes, GPTQ)
- Safetensors support
- Prometheus metrics

---

## ğŸ“Š Performance Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| TTFT (Time To First Token) | Latency to start generating | < 100ms |
| TPOT (Time Per Output Token) | Per-token generation time | < 50ms |
| Throughput | Tokens/second | Maximize |
| Memory Efficiency | Tokens served per GB VRAM | Maximize |

---

## ğŸ”¬ Advanced Techniques

### Flash Attention
Fused attention kernel with tiling for O(N) memory:

```
Standard Attention: O(NÂ²) memory (store full attention matrix)
Flash Attention: O(N) memory (tile-based, never materialize full matrix)
```

### Quantization
```
FP16: 2 bytes per param â†’ 14GB for 7B model
INT8: 1 byte per param â†’ 7GB for 7B model
INT4: 0.5 byte per param â†’ 3.5GB for 7B model
```

### Tensor Cores
Use specialized hardware for matrix ops:
- FP16: 312 TFLOPS on A100
- INT8: 624 TOPS on A100

---

## ğŸ’¼ Interview Topics

- Explain KV cache and why it's necessary
- How does PagedAttention improve memory efficiency?
- Trade-offs of different quantization levels
- Continuous batching vs static batching
- Memory breakdown of serving a 70B model
- How would you reduce TTFT?
- Speculative decoding algorithm and assumptions
