# 02. Distributed GPU Programming

## ğŸ¯ Overview
Scale your GPU workloads across multiple GPUs and multiple machines. Essential for training large models and high-throughput inference.

---

## ğŸ–¥ï¸ Multi-GPU Architectures

### Single Machine, Multiple GPUs
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Host CPU                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GPU0  â”‚  GPU1  â”‚  GPU2  â”‚  GPU3    â”‚
â”‚ PCIe   â”‚ PCIe   â”‚ PCIe   â”‚ PCIe     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†• NVLink (if available) â†•
```

### Multi-Node Cluster
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Network    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 0    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Node 1    â”‚
â”‚ GPU0  GPU1  â”‚  (InfiniBand) â”‚ GPU2  GPU3  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— NCCL (NVIDIA Collective Communications Library)

NCCL optimizes GPU-to-GPU communication:

### Key Operations

| Operation | Description | Use Case |
|-----------|-------------|----------|
| AllReduce | Sum all values, distribute result | Gradient averaging |
| Broadcast | Send from one to all | Model distribution |
| AllGather | Collect all values together | Activation sharing |
| ReduceScatter | Reduce + scatter | ZeRO optimizer |

### AllReduce Example (Gradient Sync)
```
GPU 0: [1, 2, 3]  â”€â”
GPU 1: [4, 5, 6]  â”€â”¼â”€â”€â–º AllReduce(sum) â”€â”€â”¬â”€â–º GPU 0: [5, 7, 9]
GPU 2: [0, 0, 0]  â”€â”˜                     â”œâ”€â–º GPU 1: [5, 7, 9]
                                         â””â”€â–º GPU 2: [5, 7, 9]
```

---

## ğŸ”„ Data Parallelism

Most common distributed training strategy:

```python
# PyTorch DistributedDataParallel
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# Initialize process group
dist.init_process_group(backend='nccl')

# Wrap model
model = DDP(model.to(rank), device_ids=[rank])

# Training loop (gradients auto-synchronized)
for batch in dataloader:
    loss = model(batch)
    loss.backward()  # AllReduce happens here
    optimizer.step()
```

### How It Works
1. Each GPU gets copy of model
2. Each GPU processes different data batch
3. Gradients synchronized via AllReduce
4. All GPUs update identically

---

## ğŸ§© Model Parallelism

For models too large for single GPU:

### Tensor Parallelism
Split individual layers across GPUs:
```
Linear(4096, 4096) split across 4 GPUs:
  GPU0: Linear(4096, 1024)
  GPU1: Linear(4096, 1024)
  GPU2: Linear(4096, 1024)
  GPU3: Linear(4096, 1024)
  Output: Concatenate results
```

### Pipeline Parallelism
Different layers on different GPUs:
```
GPU 0: Layers 0-10   â”€â”€â”€micro-batch-0â”€â”€â”€â–º
GPU 1: Layers 10-20  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ micro-batch-0 â”€â”€â”€â–º
GPU 2: Layers 20-30  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ micro-batch-0 â”€â”€â”€â–º
```

---

## ğŸš€ DeepSpeed

Microsoft's library for efficient large model training:

### ZeRO (Zero Redundancy Optimizer)
Reduces memory by partitioning optimizer states, gradients, and parameters:

| Stage | Partitioned | Memory Savings |
|-------|-------------|----------------|
| ZeRO-1 | Optimizer states | 4Ã— |
| ZeRO-2 | + Gradients | 8Ã— |
| ZeRO-3 | + Parameters | Linear with GPU count |

### Example Config
```json
{
  "zero_optimization": {
    "stage": 2,
    "allgather_bucket_size": 5e8,
    "reduce_bucket_size": 5e8
  },
  "fp16": {
    "enabled": true
  }
}
```

---

## ğŸ“Š Scaling Efficiency

Ideal vs real scaling:

```
GPUs:     1    2    4    8    16
Ideal:    1Ã—   2Ã—   4Ã—   8Ã—   16Ã—
Reality:  1Ã—   1.9Ã— 3.6Ã— 6.8Ã— 12Ã—
```

Overhead sources:
- Communication time
- Synchronization barriers
- Load imbalance
- Memory copies

---

## ğŸ’¼ Interview Topics

- Explain AllReduce and when to use it
- Data Parallelism vs Model Parallelism trade-offs
- What is gradient bucketing?
- How does ZeRO reduce memory?
- Debugging NCCL hangs and deadlocks
- Ring AllReduce algorithm
